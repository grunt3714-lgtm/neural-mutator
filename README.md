# Neural Mutator â€” Self-Replicating Neuroevolution

Evolve neural networks that **mutate their own weights**. Each genome contains a **policy** (plays the environment), a **mutator** (rewrites the entire genome, including itself), and a **compatibility network** (decides who can crossover). Natural selection is the only filter â€” mutators that destroy themselves die, those that improve themselves thrive.

## Architecture

![Architecture](docs/plots/architecture.png)

The system has five key components:

- **(a) Genome** â€” Three co-evolved networks: policy Î¸_p, mutator Î¸_m, and compatibility Î¸_c
- **(b) DualMixture Reproduction** â€” 80% learned corrections via encoderâ†’corrector pipeline, 20% Gaussian escape hatch (ratio evolved independently across environments, converges to ~20% universally)
- **(c) Evolutionary Loop** â€” Evaluate â†’ select â†’ speciate â†’ reproduce â†’ structural mutation
- **(d) Learned Speciation** â€” Compatibility network scores pairwise genome similarity to form breeding groups
- **(e) Learned Crossover** â€” Mutator network sees both parents and decides how to combine them

The mutator is **self-referential**: since Î¸_m âŠ‚ Î¸, the mutator rewrites its own weights through its own output.

## Results

All runs use the DualMixture mutator with flexible architecture and learned speciation.

### CartPole-v1

**Solved in 2 generations.** Best: 500 (maximum). Mean converges to ~484 by gen 50.

| | |
|---|---|
| ![CartPole Training](results/cartpole_dm_flex_s45_50g/training_plot.png) | ![CartPole Best](results/cartpole_dm_flex_s45_50g/best_gameplay.gif) |

- **Pop:** 80 Â· **Gens:** 50 Â· **Episodes:** 5 Â· **Seed:** 45
- **Final architecture:** 4â†’64â†’2 (Tanh), 386 params
- **Mutator:** 8,374 params (21.7Ã— larger than the policy it evolves)

### LunarLander-v3

**Best: 291.06** â€” well above the 200 solve threshold. Fleet-trained across 4 nodes (24 parallel workers).

| | |
|---|---|
| ![LunarLander Training](results/lunar_s45_300g_fleet/training_plot.png) | ![LunarLander Best](results/lunar_s45_300g_fleet/best_gameplay.gif) |

- **Pop:** 160 Â· **Gens:** 300 Â· **Episodes:** 10 Â· **Seed:** 45 Â· **Fleet:** 4 nodes Ã— 6 workers
- **Final architecture:** 8â†’51â†’4 (Tanh), 667 params
- **Weight analysis:** Angular velocity and angle dominate input importance â€” the network learned that orientation control is key for landing

### Acrobot-v1

**Best: -64.0** (previous baseline: -70.7). Swing-up solved efficiently.

| | |
|---|---|
| ![Acrobot Training](results/acrobot_dm_flex_s45_300g/training_plot.png) | ![Acrobot Best](results/acrobot_dm_flex_s45_300g/best_gameplay.gif) |

- **Pop:** 80 Â· **Gens:** 300 Â· **Episodes:** 10 Â· **Seed:** 45
- **Final architecture:** 6â†’64â†’64â†’3 (Tanh), 2 layers, 112 neurons
- **Fidelity:** Climbed to 0.038 â€” mutator learned stable self-replication

### Pendulum-v1

**Best: -112.3** with discrete action mapping (continuous control is inherently harder for neuroevolution). Architecture self-simplified from 128â†’48 neurons over 1000 generations.

| | |
|---|---|
| ![Pendulum Training](results/pendulum_dm_flex_s45_1000g/training_plot.png) | ![Pendulum Best](results/pendulum_dm_flex_s45_1000g/best_gameplay.gif) |

- **Pop:** 80 Â· **Gens:** 1000 Â· **Episodes:** 10 Â· **Seed:** 45
- **Final architecture:** 3â†’32â†’32â†’1 (Tanh), 2 layers â€” evolved continuous output
- **Fidelity:** 0.00 â†’ 0.23 over 1000 gens â€” mutator never stopped improving self-replication
- **Best test episode:** -10.01 (near-perfect upright balance)

### Summary

| Environment | Best Reward | Architecture | Gens | Status |
|-------------|-----------|--------------|------|--------|
| CartPole-v1 | **500** | 4â†’64â†’2, 1 layer | 50 | âœ… Solved (gen 2) |
| LunarLander-v3 | **291** | 8â†’51â†’4, 1 layer | 300 | âœ… Solved |
| Acrobot-v1 | **-64** | 6â†’64â†’64â†’3, 2 layers | 300 | âœ… Beat baseline |
| Pendulum-v1 | **-112** | 3â†’32â†’32â†’1, 2 layers | 1000 | ðŸ“ˆ Improving |

### Cross-Environment Findings

The DualMixture mutator adapts its learned parameters per environment:

- **p_gauss converges to ~20% across all environments** â€” this ratio appears to be a universal sweet spot
- **Correction scales specialize**: CartPole (0.033) > Pendulum (0.028) > LunarLander (0.020) â€” harder problems demand finer precision
- **The mutator is 12-22Ã— larger than the policy** â€” the "how to improve" knowledge is far more complex than the solution itself
- **Flexible architecture works**: networks self-compress to minimal viable size (CartPole: 128â†’64, Pendulum: 128â†’48)

## Mutator Architectures

| Mutator | Description | Crossover Mode |
|---------|-------------|----------------|
| **Gaussian** | N(0, Ïƒ) noise (ES baseline) | Random per-weight interpolation + noise |
| **Chunk MLP** | Processes weights in fixed-size chunks | Dedicated `cross_net` sees both parents' chunks |
| **Transformer** | Self-attention over weight segments | Cross-attention embedding of both parents |
| **Error Corrector** | Learned reference + targeted corrections | Encodes parent midpoint, corrects toward reference |
| **DualMixture** | NN mutator + Gaussian escape hatch (configurable p) | NN-guided crossover with Gaussian fallback |

## Usage

```bash
# Setup
python -m venv .venv
source .venv/bin/activate
pip install torch gymnasium matplotlib numpy

# Basic run
python -m src.train --env CartPole-v1 --mutator gaussian --generations 100

# Full featured run with DualMixture
python -m src.train \
    --env LunarLander-v3 \
    --mutator dualmixture \
    --flex --speciation \
    --pop-size 80 \
    --generations 300 \
    --episodes 10 \
    --workers 6

# Fleet training (distributed across nodes)
python -m src.train \
    --env LunarLander-v3 \
    --mutator dualmixture \
    --flex --speciation \
    --pop-size 160 --generations 300 --episodes 10 \
    --fleet --fleet-port 5611 --fleet-workers 4
# Then on each worker node:
python -m fleet.worker --host <manager-ip> --port 5611 --workers 6 --name node1
```

## Key Properties

- **Self-referential**: The mutator modifies itself through its own output
- **Evolvable variation operator**: Natural selection acts on the mutation strategy, not just the policy
- **Learned recombination**: Crossover is performed by the mutator network, not fixed rules
- **Adaptive precision**: Correction scales evolve per-environment â€” tight for hard, loose for easy
- **Parsimony pressure**: Flex architecture + selection drives evolution toward minimal networks

## Related Work

- Neural Network Quine (Chang & Lipson, 2018) â€” self-replicating networks
- HyperNEAT (Stanley et al., 2009) â€” CPPNs for indirect weight encoding
- Adaptive RL through Evolving Self-Modifying NNs (Schmidgall, 2020)
- Self-Referential Meta Learning (Kirsch & Schmidhuber, 2022)

## License

MIT
